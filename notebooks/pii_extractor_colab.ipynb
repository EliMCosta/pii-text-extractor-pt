{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PII Text Extractor - Pipeline no Google Colab\n",
        "\n",
        "Este notebook permite executar o pipeline completo de extração de PII (Dados Pessoais) diretamente no Google Colab.\n",
        "\n",
        "**Funcionalidades:**\n",
        "- Clonagem do repositório do GitHub\n",
        "- Instalação de dependências\n",
        "- Download do dataset\n",
        "- Preparação dos dados (chunking)\n",
        "- Fine-tuning do modelo\n",
        "- Inferência em textos\n",
        "- Avaliação do modelo\n",
        "\n",
        "**Recomendação:** Use um runtime com GPU (T4 ou superior) para treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuração do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title Configurações {display-mode: \"form\"}\n",
        "\n",
        "# URL do repositório no GitHub\n",
        "GITHUB_REPO = \"https://github.com/EliMCosta/pii-text-extractor-pt.git\"  # @param {type:\"string\"}\n",
        "\n",
        "# Branch a ser clonada\n",
        "GITHUB_BRANCH = \"main\"  # @param {type:\"string\"}\n",
        "\n",
        "# Diretório de trabalho\n",
        "WORK_DIR = \"/content/pii-text-extractor-pt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verificar GPU disponível\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Clonar o repositório do GitHub\n",
        "import os\n",
        "\n",
        "if os.path.exists(WORK_DIR):\n",
        "    print(f\"Diretório {WORK_DIR} já existe. Atualizando...\")\n",
        "    %cd {WORK_DIR}\n",
        "    !git pull origin {GITHUB_BRANCH}\n",
        "else:\n",
        "    !git clone --branch {GITHUB_BRANCH} {GITHUB_REPO} {WORK_DIR}\n",
        "    %cd {WORK_DIR}\n",
        "\n",
        "print(f\"\\nDiretório atual: {os.getcwd()}\")\n",
        "!ls -la"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Instalar dependências usando uv (muito mais rápido que pip)\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# Instalar uv\n",
        "!pip install -q uv\n",
        "\n",
        "# Corrigir incompatibilidade torch/torchvision do Colab\n",
        "!uv pip install --system --quiet torchvision --index-url https://download.pytorch.org/whl/cu121 --reinstall-package torchvision\n",
        "\n",
        "# Instalar dependências do projeto\n",
        "!uv pip install --system --quiet -r requirements.txt\n",
        "\n",
        "print(\"Dependências instaladas com sucesso!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Criar diretório de dados e clonar o dataset do Hugging Face\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!mkdir -p data\n",
        "%cd data\n",
        "\n",
        "if os.path.exists(\"esic-ner\"):\n",
        "    print(\"Dataset já existe. Atualizando...\")\n",
        "    %cd esic-ner\n",
        "    !git pull\n",
        "    %cd ..\n",
        "else:\n",
        "    !git clone https://huggingface.co/datasets/EliMC/esic-ner\n",
        "\n",
        "%cd {WORK_DIR}\n",
        "print(\"\\nArquivos do dataset:\")\n",
        "!ls -la data/esic-ner/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preparação dos Dados (Smart Chunking)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Processar o dataset com Smart Chunking\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python data_preprocessing/build_finetune_jsonl.py \\\n",
        "    --input data/esic-ner/train.jsonl \\\n",
        "    --output data/esic-ner/train_chunks.jsonl \\\n",
        "    --max_length 512 \\\n",
        "    --stride 64\n",
        "\n",
        "print(\"\\nArquivo processado:\")\n",
        "!ls -lh data/esic-ner/train_chunks.jsonl\n",
        "!wc -l data/esic-ner/train_chunks.jsonl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fine-tuning do Modelo\n",
        "\n",
        "**Atenção:** Esta etapa pode levar vários minutos dependendo do tamanho do dataset e da GPU disponível."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title Parâmetros de Treinamento {display-mode: \"form\"}\n",
        "\n",
        "MODEL_BASE = \"neuralmind/bert-base-portuguese-cased\"  # @param {type:\"string\"}\n",
        "NUM_EPOCHS = 3  # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 8  # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 5e-5  # @param {type:\"number\"}\n",
        "OUTPUT_DIR = \"outputs/pii-textx-pt\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Executar o fine-tuning\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 python training/finetune_pii_token_classification.py \\\n",
        "    --model_name_or_path {MODEL_BASE} \\\n",
        "    --dataset_path data/esic-ner/train_chunks.jsonl \\\n",
        "    --output_dir {OUTPUT_DIR} \\\n",
        "    --num_train_epochs {NUM_EPOCHS} \\\n",
        "    --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "    --learning_rate {LEARNING_RATE} \\\n",
        "    --bf16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Verificar o modelo treinado\n",
        "print(\"Arquivos do modelo treinado:\")\n",
        "!ls -la {OUTPUT_DIR}/best/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inferência\n",
        "\n",
        "Agora você pode usar o modelo treinado para extrair PII de textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title Inferência em Texto {display-mode: \"form\"}\n",
        "\n",
        "TEXTO_EXEMPLO = \"O CPF do solicitante João Silva é 123.456.789-00. Ele mora na Rua das Flores, 123.\"  # @param {type:\"string\"}\n",
        "MODEL_PATH = \"outputs/pii-textx-pt/best\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Executar inferência via CLI\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python infer_pii.py \\\n",
        "    --model_name_or_path {MODEL_PATH} \\\n",
        "    infer \\\n",
        "    --text \"{TEXTO_EXEMPLO}\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Executar avaliação no dataset de teste\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# Verificar se existe arquivo de teste\n",
        "import os\n",
        "test_file = \"data/esic-ner/test.jsonl\"\n",
        "if os.path.exists(test_file):\n",
        "    !python infer_pii.py \\\n",
        "        --model_name_or_path {MODEL_PATH} \\\n",
        "        eval \\\n",
        "        --dataset_path {test_file} \\\n",
        "        --report_path outputs/eval_report.md\n",
        "else:\n",
        "    print(f\"Arquivo de teste não encontrado: {test_file}\")\n",
        "    print(\"Usando uma amostra do train.jsonl para demonstração...\")\n",
        "    !head -100 data/esic-ner/train.jsonl > data/esic-ner/sample_test.jsonl\n",
        "    !python infer_pii.py \\\n",
        "        --model_name_or_path {MODEL_PATH} \\\n",
        "        eval \\\n",
        "        --dataset_path data/esic-ner/sample_test.jsonl \\\n",
        "        --report_path outputs/eval_report.md \\\n",
        "        --max_rows 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualizar relatório de avaliação\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "report_path = f\"{WORK_DIR}/outputs/eval_report.md\"\n",
        "if os.path.exists(report_path):\n",
        "    with open(report_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        display(Markdown(f.read()))\n",
        "else:\n",
        "    print(\"Relatório não encontrado.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Salvar Modelo no Google Drive (Opcional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Copiar modelo para o Drive\n",
        "DRIVE_OUTPUT_PATH = \"/content/drive/MyDrive/pii-extractor-model\"  # @param {type:\"string\"}\n",
        "\n",
        "!mkdir -p \"{DRIVE_OUTPUT_PATH}\"\n",
        "!cp -r {WORK_DIR}/{OUTPUT_DIR}/best/* \"{DRIVE_OUTPUT_PATH}/\"\n",
        "\n",
        "print(f\"Modelo salvo em: {DRIVE_OUTPUT_PATH}\")\n",
        "!ls -la \"{DRIVE_OUTPUT_PATH}/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inferência em Lote (JSONL)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Criar arquivo de exemplo para inferência em lote\n",
        "import json\n",
        "\n",
        "exemplos = [\n",
        "    {\"text\": \"Meu nome é Maria da Silva e meu CPF é 987.654.321-00.\"},\n",
        "    {\"text\": \"O processo SEI 00400-00123456/2024-99 foi instaurado.\"},\n",
        "    {\"text\": \"Solicito informações sobre a Lei 12.527/2011.\"},\n",
        "    {\"text\": \"Entre em contato pelo e-mail joao.santos@email.com ou telefone (61) 99999-8888.\"},\n",
        "]\n",
        "\n",
        "input_file = f\"{WORK_DIR}/data/exemplos_inferencia.jsonl\"\n",
        "with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in exemplos:\n",
        "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Arquivo criado: {input_file}\")\n",
        "!cat {input_file}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Executar inferência em lote\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python infer_pii.py \\\n",
        "    --model_name_or_path {MODEL_PATH} \\\n",
        "    infer \\\n",
        "    --jsonl_in data/exemplos_inferencia.jsonl \\\n",
        "    --jsonl_out outputs/resultados_inferencia.jsonl\n",
        "\n",
        "print(\"\\nResultados:\")\n",
        "!cat outputs/resultados_inferencia.jsonl | python -m json.tool --no-ensure-ascii"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Usando um Modelo Pré-treinado do Hugging Face Hub\n",
        "\n",
        "Se você tiver um modelo já publicado no Hugging Face Hub, pode usá-lo diretamente sem precisar treinar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# @title Usar Modelo do Hugging Face Hub {display-mode: \"form\"}\n",
        "\n",
        "HF_MODEL_ID = \"EliMC/pii-text-extractor-pt\"  # @param {type:\"string\"}\n",
        "TEXTO_TESTE = \"O contribuinte José Santos, CPF 111.222.333-44, solicita revisão.\"  # @param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Inferência com modelo do Hub\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python infer_pii.py \\\n",
        "    --model_name_or_path {HF_MODEL_ID} \\\n",
        "    infer \\\n",
        "    --text \"{TEXTO_TESTE}\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}