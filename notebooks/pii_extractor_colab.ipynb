{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PII Text Extractor - Pipeline no Google Colab\n",
        "\n",
        "Este notebook permite executar o pipeline completo de extração de PII (Dados Pessoais) diretamente no Google Colab.\n",
        "\n",
        "**Funcionalidades:**\n",
        "- Clonagem do repositório do GitHub\n",
        "- Instalação de dependências\n",
        "- Download do dataset\n",
        "- Preparação dos dados (chunking)\n",
        "- Fine-tuning do modelo\n",
        "- Inferência em textos\n",
        "- Avaliação do modelo\n",
        "\n",
        "**Recomendação:** Use um runtime com GPU (T4 ou superior) para treino."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuração do Ambiente"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Configurações {display-mode: \"form\"}\n",
        "\n",
        "# URL do repositório no GitHub\n",
        "GITHUB_REPO = \"https://github.com/EliMCosta/pii-text-extractor-pt.git\"  # @param {type:\"string\"}\n",
        "\n",
        "# Branch a ser clonada\n",
        "GITHUB_BRANCH = \"main\"  # @param {type:\"string\"}\n",
        "\n",
        "# Diretório de trabalho\n",
        "WORK_DIR = \"/content/pii-text-extractor-pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar GPU disponível\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clonar o repositório do GitHub\n",
        "import os\n",
        "\n",
        "if os.path.exists(WORK_DIR):\n",
        "    print(f\"Diretório {WORK_DIR} já existe. Atualizando...\")\n",
        "    %cd {WORK_DIR}\n",
        "    !git pull origin {GITHUB_BRANCH}\n",
        "else:\n",
        "    !git clone --branch {GITHUB_BRANCH} {GITHUB_REPO} {WORK_DIR}\n",
        "    %cd {WORK_DIR}\n",
        "\n",
        "print(f\"\\nDiretório atual: {os.getcwd()}\")\n",
        "!ls -la"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Instalar dependências\n",
        "# Nota: Warnings de conflitos com pacotes pré-instalados do Colab podem ser ignorados\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!pip install -q --upgrade -r requirements.txt 2>&1 | grep -v \"dependency conflicts\"\n",
        "print(\"Dependências instaladas com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download do Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar diretório de dados e clonar o dataset do Hugging Face\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!mkdir -p data\n",
        "%cd data\n",
        "\n",
        "if os.path.exists(\"esic-ner\"):\n",
        "    print(\"Dataset já existe. Atualizando...\")\n",
        "    %cd esic-ner\n",
        "    !git pull\n",
        "    %cd ..\n",
        "else:\n",
        "    !git clone https://huggingface.co/datasets/EliMC/esic-ner\n",
        "\n",
        "%cd {WORK_DIR}\n",
        "print(\"\\nArquivos do dataset:\")\n",
        "!ls -la data/esic-ner/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preparação dos Dados (Smart Chunking)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Processar o dataset com Smart Chunking\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python data_preprocessing/build_finetune_jsonl.py \\\n",
        "    --input data/esic-ner/train.jsonl \\\n",
        "    --output data/esic-ner/train_chunks.jsonl \\\n",
        "    --max_length 512 \\\n",
        "    --stride 64\n",
        "\n",
        "print(\"\\nArquivo processado:\")\n",
        "!ls -lh data/esic-ner/train_chunks.jsonl\n",
        "!wc -l data/esic-ner/train_chunks.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Fine-tuning do Modelo\n",
        "\n",
        "**Atenção:** Esta etapa pode levar vários minutos dependendo do tamanho do dataset e da GPU disponível."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Parâmetros de Treinamento {display-mode: \"form\"}\n",
        "\n",
        "MODEL_BASE = \"neuralmind/bert-base-portuguese-cased\"  # @param {type:\"string\"}\n",
        "NUM_EPOCHS = 3  # @param {type:\"integer\"}\n",
        "BATCH_SIZE = 8  # @param {type:\"integer\"}\n",
        "LEARNING_RATE = 5e-5  # @param {type:\"number\"}\n",
        "OUTPUT_DIR = \"outputs/pii-textx-pt\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executar o fine-tuning\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0 accelerate launch --num_processes 1 \\\n",
        "    training/finetune_pii_token_classification.py \\\n",
        "    --model_name_or_path {MODEL_BASE} \\\n",
        "    --dataset_path data/esic-ner/train_chunks.jsonl \\\n",
        "    --output_dir {OUTPUT_DIR} \\\n",
        "    --num_train_epochs {NUM_EPOCHS} \\\n",
        "    --per_device_train_batch_size {BATCH_SIZE} \\\n",
        "    --learning_rate {LEARNING_RATE} \\\n",
        "    --bf16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar o modelo treinado\n",
        "print(\"Arquivos do modelo treinado:\")\n",
        "!ls -la {OUTPUT_DIR}/best/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Inferência\n",
        "\n",
        "Agora você pode usar o modelo treinado para extrair PII de textos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Inferência em Texto {display-mode: \"form\"}\n",
        "\n",
        "TEXTO_EXEMPLO = \"O CPF do solicitante João Silva é 123.456.789-00. Ele mora na Rua das Flores, 123.\"  # @param {type:\"string\"}\n",
        "MODEL_PATH = \"outputs/pii-textx-pt/best\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executar inferência via CLI\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python infer_pii.py \\\n",
        "    --model_name_or_path {MODEL_PATH} \\\n",
        "    infer \\\n",
        "    --text \"{TEXTO_EXEMPLO}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inferência via Python (para integração programática)\n",
        "import sys\n",
        "sys.path.insert(0, WORK_DIR)\n",
        "\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
        "from data_preprocessing.chunking import build_chunks\n",
        "from inference import (\n",
        "    get_label_maps_from_model,\n",
        "    viterbi_decode_bio,\n",
        "    spans_from_token_predictions_scored,\n",
        "    filter_scored_spans,\n",
        "    merge_and_resolve_scored_spans,\n",
        ")\n",
        "from ner_labels import PII_TYPES\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def infer_pii(text: str, model_path: str = \"outputs/pii-textx-pt/best\") -> dict:\n",
        "    \"\"\"Executa inferência de PII em um texto.\"\"\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
        "    model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    label2id, id2label = get_label_maps_from_model(model)\n",
        "    o_id = int(label2id[\"O\"])\n",
        "    \n",
        "    chunks = build_chunks(\n",
        "        text=text,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=512,\n",
        "        stride=64,\n",
        "        boundary_backoff=32,\n",
        "    )\n",
        "    \n",
        "    token_logits_sum = {}\n",
        "    token_logits_count = {}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for ch in chunks:\n",
        "            enc = tokenizer(\n",
        "                ch.text,\n",
        "                add_special_tokens=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                padding=True,\n",
        "                return_offsets_mapping=True,\n",
        "                return_tensors=\"pt\",\n",
        "            )\n",
        "            offsets = enc.pop(\"offset_mapping\")[0].tolist()\n",
        "            attn = enc.get(\"attention_mask\")[0].tolist()\n",
        "            enc = {k: v.to(device) for k, v in enc.items()}\n",
        "            logits = model(**enc).logits[0].cpu().numpy()\n",
        "            \n",
        "            for ti, (a, b) in enumerate(offsets):\n",
        "                if a == 0 and b == 0:\n",
        "                    continue\n",
        "                if int(attn[ti]) == 0:\n",
        "                    continue\n",
        "                ga = int(a) + int(ch.char_start)\n",
        "                gb = int(b) + int(ch.char_start)\n",
        "                if ga >= gb:\n",
        "                    continue\n",
        "                k = (ga, gb)\n",
        "                v = logits[ti].astype(np.float32)\n",
        "                if k in token_logits_sum:\n",
        "                    token_logits_sum[k] += v\n",
        "                    token_logits_count[k] += 1\n",
        "                else:\n",
        "                    token_logits_sum[k] = v.copy()\n",
        "                    token_logits_count[k] = 1\n",
        "    \n",
        "    if not token_logits_sum:\n",
        "        return {\"text\": text, \"spans\": [], \"should_be_public\": True}\n",
        "    \n",
        "    keys = sorted(token_logits_sum.keys())\n",
        "    em_global = np.stack(\n",
        "        [token_logits_sum[k] / token_logits_count[k] for k in keys]\n",
        "    ).astype(np.float32)\n",
        "    offs_global = list(keys)\n",
        "    \n",
        "    pred_ids = viterbi_decode_bio(\n",
        "        emissions=em_global,\n",
        "        id2label=id2label,\n",
        "        o_id=o_id,\n",
        "        force_o_mask=None,\n",
        "    )\n",
        "    \n",
        "    spans_scored = spans_from_token_predictions_scored(\n",
        "        offsets=offs_global,\n",
        "        pred_ids=pred_ids,\n",
        "        logits=em_global,\n",
        "        id2label=id2label,\n",
        "        o_id=o_id,\n",
        "        conf_agg=\"mean\",\n",
        "    )\n",
        "    spans_scored = filter_scored_spans(spans_scored, conf_threshold=0.0)\n",
        "    merged = merge_and_resolve_scored_spans(spans_scored, resolve_overlaps=True)\n",
        "    \n",
        "    pii_types = set(PII_TYPES)\n",
        "    should_be_public = not any(s.pii_type in pii_types for s in merged)\n",
        "    \n",
        "    return {\n",
        "        \"text\": text,\n",
        "        \"spans\": [\n",
        "            {\n",
        "                \"type\": s.pii_type,\n",
        "                \"start\": s.start,\n",
        "                \"end\": s.end,\n",
        "                \"value\": text[s.start:s.end],\n",
        "                \"confidence\": round(s.confidence, 4),\n",
        "            }\n",
        "            for s in merged\n",
        "        ],\n",
        "        \"should_be_public\": should_be_public,\n",
        "    }\n",
        "\n",
        "\n",
        "# Testar\n",
        "resultado = infer_pii(TEXTO_EXEMPLO, MODEL_PATH)\n",
        "print(json.dumps(resultado, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Avaliação do Modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executar avaliação no dataset de teste\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "# Verificar se existe arquivo de teste\n",
        "import os\n",
        "test_file = \"data/esic-ner/test.jsonl\"\n",
        "if os.path.exists(test_file):\n",
        "    !python infer_pii.py \\\n",
        "        --model_name_or_path {MODEL_PATH} \\\n",
        "        eval \\\n",
        "        --dataset_path {test_file} \\\n",
        "        --report_path outputs/eval_report.md\n",
        "else:\n",
        "    print(f\"Arquivo de teste não encontrado: {test_file}\")\n",
        "    print(\"Usando uma amostra do train.jsonl para demonstração...\")\n",
        "    !head -100 data/esic-ner/train.jsonl > data/esic-ner/sample_test.jsonl\n",
        "    !python infer_pii.py \\\n",
        "        --model_name_or_path {MODEL_PATH} \\\n",
        "        eval \\\n",
        "        --dataset_path data/esic-ner/sample_test.jsonl \\\n",
        "        --report_path outputs/eval_report.md \\\n",
        "        --max_rows 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizar relatório de avaliação\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "report_path = f\"{WORK_DIR}/outputs/eval_report.md\"\n",
        "if os.path.exists(report_path):\n",
        "    with open(report_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        display(Markdown(f.read()))\n",
        "else:\n",
        "    print(\"Relatório não encontrado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Salvar Modelo no Google Drive (Opcional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copiar modelo para o Drive\n",
        "DRIVE_OUTPUT_PATH = \"/content/drive/MyDrive/pii-extractor-model\"  # @param {type:\"string\"}\n",
        "\n",
        "!mkdir -p \"{DRIVE_OUTPUT_PATH}\"\n",
        "!cp -r {WORK_DIR}/{OUTPUT_DIR}/best/* \"{DRIVE_OUTPUT_PATH}/\"\n",
        "\n",
        "print(f\"Modelo salvo em: {DRIVE_OUTPUT_PATH}\")\n",
        "!ls -la \"{DRIVE_OUTPUT_PATH}/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inferência em Lote (JSONL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Criar arquivo de exemplo para inferência em lote\n",
        "import json\n",
        "\n",
        "exemplos = [\n",
        "    {\"text\": \"Meu nome é Maria da Silva e meu CPF é 987.654.321-00.\"},\n",
        "    {\"text\": \"O processo SEI 00400-00123456/2024-99 foi instaurado.\"},\n",
        "    {\"text\": \"Solicito informações sobre a Lei 12.527/2011.\"},\n",
        "    {\"text\": \"Entre em contato pelo e-mail joao.santos@email.com ou telefone (61) 99999-8888.\"},\n",
        "]\n",
        "\n",
        "input_file = f\"{WORK_DIR}/data/exemplos_inferencia.jsonl\"\n",
        "with open(input_file, \"w\", encoding=\"utf-8\") as f:\n",
        "    for ex in exemplos:\n",
        "        f.write(json.dumps(ex, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "print(f\"Arquivo criado: {input_file}\")\n",
        "!cat {input_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Executar inferência em lote\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python infer_pii.py \\\n",
        "    --model_name_or_path {MODEL_PATH} \\\n",
        "    infer \\\n",
        "    --jsonl_in data/exemplos_inferencia.jsonl \\\n",
        "    --jsonl_out outputs/resultados_inferencia.jsonl\n",
        "\n",
        "print(\"\\nResultados:\")\n",
        "!cat outputs/resultados_inferencia.jsonl | python -m json.tool --no-ensure-ascii"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Usando um Modelo Pré-treinado do Hugging Face Hub\n",
        "\n",
        "Se você tiver um modelo já publicado no Hugging Face Hub, pode usá-lo diretamente sem precisar treinar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Usar Modelo do Hugging Face Hub {display-mode: \"form\"}\n",
        "\n",
        "HF_MODEL_ID = \"EliMC/pii-text-extractor-pt\"  # @param {type:\"string\"}\n",
        "TEXTO_TESTE = \"O contribuinte José Santos, CPF 111.222.333-44, solicita revisão.\"  # @param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inferência com modelo do Hub\n",
        "%cd {WORK_DIR}\n",
        "\n",
        "!python infer_pii.py \\\n",
        "    --model_name_or_path {HF_MODEL_ID} \\\n",
        "    infer \\\n",
        "    --text \"{TEXTO_TESTE}\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
